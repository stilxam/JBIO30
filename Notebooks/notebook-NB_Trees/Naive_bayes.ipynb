{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn\n",
    "#print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================================================\n",
    "# Lecture Notes: Naive Bayes\n",
    "\n",
    "\n",
    "##### D.Vidotto, M. Shafiee Kamalabad, S. Hess, Data Mining: JBI030 2020/20201\n",
    "\n",
    "\n",
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a **simple**, yet computationally **fast** and often **effective**, probabilistic classification algorithm. \n",
    "\n",
    "Its **name** is due to the fact that it relies on **Bayes' theorem**; it is **'naive'** as it naively assumes that all features are **conditionally independent** with each other. Although this is a strong assumption that rarely holds, this algorithm can still perform well, especially in large dimensions. \n",
    "\n",
    "\n",
    "In this notebook, we are going to review the following concepts: \n",
    "1. Probability Review \n",
    "1. Naive Bayes\n",
    "  * Categorical Naive Bayes \n",
    "  * Count smoothing\n",
    "  * Multinomial Naive Bayes \n",
    "  * Gaussian Naive Bayes\n",
    "1. Examples in Python\n",
    "1. Example with the Heart Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Review \n",
    "\n",
    "**Probability: Definition** According the the three *axioms of probability*, we can define a probability such as that quantity $P$ for which the following three axioms are always true: \n",
    "1. $0 \\leq P \\leq 1$\n",
    "1. $P(A \\cup B) = P(A) + P(B)$ (if $A$ and $B$ are *disjoint events*) \n",
    "1. $P(S) = 1$ (where $S$ is the whole *sampling space*) \n",
    "\n",
    "We can **estimate a probability** of an event as the **fraction** of times that the event occurs in the sampling space.\n",
    "\n",
    "For instance, if we had a urn with 5 green balls, 3 blue balls, and 2 red balls (10 balls in total): \n",
    "\n",
    "<img src=\"./img/naive_bayes/prob_1.png\" width=\"200\" height=\"50\"/>\n",
    "\n",
    "Then we can estimate $\\Pr(Green)=1/2$, $\\Pr(Blue)=3/10$, $\\Pr(Red)=1/5$. \n",
    "\n",
    "<img src=\"./img/naive_bayes/pr_color.png\" width=\"200\" height=\"50\"/>\n",
    "\n",
    "Note that, for the second axiom above, $\\Pr(Red\\ OR\\ Blue) = 1/5+3/10=1/2$, while for the third axiom $\\Pr(Red\\ OR\\ Blue\\ OR\\ Green) = 1$. For this last axiom, it is also implied that $\\Pr(Red)=1-\\Pr(Red^C)=1-(1/2+3/10)=1/5$.\n",
    "\n",
    "If we had **continuous** data, we can for example estimate their **mean** and **standard deviation**, and assume a probability distribution such as the Gaussian to estimate their **density** (note that the density function of continuous probability distributions do not return probabilities: they can be larger than 1). \n",
    "\n",
    "The density of the Gaussian evaluated at a point $x$ is defined as: \n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "The values of $\\sigma^2$ and $\\mu$ can be replaced with the estimate of the mean, $\\hat{\\mu} = \\bar{x}$ and the square of the estimate of the standard deviation, $\\hat{\\sigma}^2$. \n",
    "\n",
    "<img src=\"./img/naive_bayes/pr_continuous.png\" width=\"325\" height=\"50\"/>\n",
    "\n",
    "**Joint Probability**. If we had data for $p>1$ (random) variables (r.v.) $X_1,...,X_p$, we can estimate the *joint probability* $\\Pr(X_1,...,X_p)$ of such variables. For categorical data, consider the following table (here we consider $p=2$ variables, but the extension to more variables is straightforward): \n",
    "\n",
    "\n",
    "| Sex | Marital Status |\n",
    "|:------:|:-----:|\n",
    "| M | Married | \n",
    "| F | Married |  \n",
    "| F | Single | \n",
    "| M | Single |\n",
    "| F | Married | \n",
    "\n",
    "We have the following *marginal probabilities* (univariate probabilities): \n",
    "* $\\Pr(Sex=M) = 2/5 $\n",
    "* $\\Pr(Sex=F) = 3/5$\n",
    "* $\\Pr(M.Status = Married) = 3/5$\n",
    "* $\\Pr(M.Status = Single) = 2/5$\n",
    "\n",
    "and the following *joint probabilities*: \n",
    "* $\\Pr(Sex=M, M.Status = Married) = 1/5 $\n",
    "* $\\Pr(Sex=M, M.Status = Single) = 1/5 $\n",
    "* $\\Pr(Sex=F, M.Status = Married) = 2/5 $\n",
    "* $\\Pr(Sex=F, M.Status = Single) = 1/5 $\n",
    "\n",
    "**Univariate probabilities** are **called marginals** because they can be retrieved by summing the joint probabilities over all values of other variables. For example: \n",
    "\n",
    "$$\\Pr(Sex=M) = \\Pr(Sex=M, M.Status = Married) + \\Pr(Sex=M, M.Status = Single).$$\n",
    "\n",
    "In case of continuous data, we can calculate the variable-specific averages, the *covariance matrix*  of the variables, and impose multivariate continuous distributions, such as the multivariate Gaussian (for example). Here, we can see an example with $p=2$ variables, $X_1$ and $X_2$: \n",
    "\n",
    "<img src=\"./img/naive_bayes/joint_probability.png\" width=\"650\" height=\"50\"/>\n",
    "\n",
    "**Conditional Probability**. Given two (non-disjoint) events $A$ and $B$, the *conditional probability* of $A$ given $B$ is:\n",
    "\n",
    "$$\\Pr(A|B) = \\frac{\\Pr(A,B)}{\\Pr(B)}$$. \n",
    "\n",
    "For three events $A$, $B$, and $C$, the conditional probability of $A,B$ given $C$ is: \n",
    "\n",
    "$$\\Pr(A,B|C) = \\frac{\\Pr(A,B, C)}{\\Pr(C)}$$\n",
    "\n",
    "and similarly, for $p$ r.v.'s, the conditional distribution is defined as, \n",
    "\n",
    "$$\\Pr(X_1,...,X_{p-1}|X_p) = \\frac{\\Pr(X_1,..,X_{p-1}, X_p)}{\\Pr(X_p)}.$$ \n",
    "\n",
    "Note also that \n",
    "\n",
    "\n",
    "<br>\n",
    "$\\Pr(X_p)\\Pr(X_1,...,X_{p-1}|X_p) = \\Pr(X_1,...,X_p)$.\n",
    "<br>\n",
    "\n",
    "\n",
    "Coming back to the *Sex*, *Marital Status* table, we can estimate the following probabilities of the marital status, conditioned on $Sex=M$:\n",
    "\n",
    "| Sex | Marital Status |\n",
    "|:------:|:-----:|\n",
    "| M | Married | \n",
    "| F | Married |  \n",
    "| F | Single | \n",
    "| M | Single |\n",
    "| F | Married | \n",
    "\n",
    "\n",
    "\n",
    "* $\\Pr(M.Status = Married|Sex=M) = \\frac{1/5}{2/5} = \\frac{1}{2}$\n",
    "* $\\Pr(M.Status = Single|Sex=M) = \\frac{1/5}{2/5} = \\frac{1}{2}$\n",
    "\n",
    "**Continuous** r.v.'s can be also **conditioned**, either on other **continuous variables**, or on **categorical** ones. In the following plot, you can see an example of conditioning a r.v. $X$ (weights or heights in a population) on two groups, defined by the Sex of the units. \n",
    "\n",
    "On the left, you see the Gaussian of the *marginal* distribution of $X$, while on the right you see the group-specific (conditional) Gaussians.  \n",
    "\n",
    "<img src=\"./img/naive_bayes/conditional_gaussian.png\" width=\"650\" height=\"50\"/>\n",
    "\n",
    "**Independence**. Two events $A$ and $B$ are said to be *independent* if \n",
    "\n",
    "$$\\Pr(A,B) = \\Pr(A)\\Pr(B)$$\n",
    "\n",
    "that is, their joint probability is equal to the product of their marginal probabilities. (Independence is denoted with $A{\\perp\\!\\!\\!\\perp}B$.) This means that \n",
    "\n",
    "$$\\Pr(A|B) = \\Pr(A).$$\n",
    "\n",
    "Two r.v.'s $X_1$ and $X_2$ are independent if $\\Pr(X_1=x_1, X_2=x_2) =\\Pr(X_1=x_1)\\Pr(X_2=x_2)$ for all $x_1, x_2$ of the sampling space. We can easily extend the concept of independence to more than two events and r.v.'s. In case of random variables, $X_1$, $X_2$ and $X_3$ are independent with each other iff: \n",
    "\n",
    "$$\\Pr(X_1, X_2, X_3) = \\Pr(X_1)\\Pr(X_2)\\Pr(X_3)$$\n",
    "\n",
    "for all $x_1$, $x_2$, $x_3$ of their sampling space. \n",
    "\n",
    "Let's go back again to the example of the table of *Sex* and *Marital Status*; in this case, Sex and Marital Status are not independent. For example: \n",
    "\n",
    "$$\\Pr(M.Status = Married,Sex=M) = 1/5$$\n",
    "$$\\Pr(M.Status = Married)\\Pr(Sex = M) = 3/5\\cdot2/5 = 6/25$$\n",
    "\n",
    "and therefore independence does not hold. This can also be seen by drawing the *barplot* of the distribution of *Marital Status*, marginal and conditioned on *Sex*: \n",
    "\n",
    "<img src=\"./img/naive_bayes/conditional_categorical.png\" width=\"650\" height=\"50\"/>\n",
    "\n",
    "As you can see, the marginal distribution of *Marital Status* is different from the ones observed for males and females. This means that the probability of *Marital Status* depends on the subject's *Sex*. However, if we had a contingency table that led to the following situation: \n",
    "\n",
    "<img src=\"./img/naive_bayes/categorical_independent.png\" width=\"650\" height=\"50\"/>\n",
    "\n",
    "then we would have two independent r.v.'s. \n",
    "\n",
    "In case of **Gaussian data**, we can **examine** the **covariance matrix** of the r.v.'s. If covariances (or equivalently the correlations) are **equal to 0** (and thus we have a **diagonal covariance matrix**), then we **do not have linear dependence** among the variables; if covariances are **different from 0**, than variables are said to be **linearly correlated**. \n",
    "\n",
    "**Notice** that lack of correlation does not imply lack of dependence, as other types of relationships may be possible. In this plot, you can see two possible relationships for the bivariate ($X_1,X_2$) case: independent variables, and correlated variables. \n",
    "\n",
    "**Notice** that, in case of diagonal covariance matrix, the shape of the Gaussian distribution is *axis-aligned*, but it is not in case of linear relationships (non-diagonal covariance matrix). \n",
    "\n",
    "<img src=\"./img/naive_bayes/indep_dep_continuous.png\" width=\"650\" height=\"50\"/>\n",
    "\n",
    "\n",
    "**Conditional Independence**. Two events $A$ and $B$ are conditionally independent given a third event $C$ (denoted with $A{\\perp\\!\\!\\!\\perp}B|C$) iff. \n",
    "\n",
    "$$\\Pr(A,B|C) = \\Pr(A|C)\\Pr(B|C)$$\n",
    "\n",
    "Similarly, the concept can be formulated for r.v.'s: \n",
    "\n",
    "$$\\Pr(X_1,X_2|X_3) = \\Pr(X_1|X_3)\\Pr(X_2|X_3)$$\n",
    "\n",
    "for all values of $x_1, x_2, x_3$. The concept can be extended to multiple events or r.v.'s. In case of 4 r.v.'s $X_1,...,X_4$ given $X_5$:\n",
    "\n",
    "$$\\Pr(X_1,X_2,X_3,X_4|X_5) = \\Pr(X_1|X_5)\\Pr(X_2|X_5)\\Pr(X_3|X_5)\\Pr(X_4|X_5).$$\n",
    "\n",
    "\n",
    "Consider the following table: \n",
    "\n",
    "| Sex | Marital Status | Education | \n",
    "|:------:|:-----:|:-----:|\n",
    "| M | Single | High School (HS) | \n",
    "| F | Single | Primary School (PS) |  \n",
    "| F | Single | HS |  \n",
    "| M | Married | PS |\n",
    "| M | Single | PS |\n",
    "| F | Married | HS |\n",
    "| M | Married | HS | \n",
    "| F | Married | PS | \n",
    "\n",
    "Let's check if *Marital Status* is independent of *Education*, conditional on *Sex*: \n",
    "* $\\Pr(Married,PS|M) = 1/4 = \\Pr(Married|M)\\Pr(PS|M) $\n",
    "* $\\Pr(Married,HS|M) = 1/4 = \\Pr(Married|M)\\Pr(HS|M) $\n",
    "* $\\Pr(Single, PS|M) = 1/4 = \\Pr(Single|M)\\Pr(PS|M) $\n",
    "* $\\Pr(Single, HS|M) = 1/4 = \\Pr(Single|M)\\Pr(HS|M) $\n",
    "* $\\Pr(Married,PS|F) = 1/4 = \\Pr(Married|M)\\Pr(PS|F) $\n",
    "* $\\Pr(Married,HS|F) = 1/4 = \\Pr(Married|M)\\Pr(HS|F) $\n",
    "* $\\Pr(Single, PS|F) = 1/4 = \\Pr(Single|M)\\Pr(PS|F) $\n",
    "* $\\Pr(Single, HS|F) = 1/4 = \\Pr(Single|M)\\Pr(HS|F) $\n",
    "\n",
    "Thus, the two variables are conditionally independent! (Check the results by yourself.)\n",
    "\n",
    "Also continuous variables can become conditionally independent. In the following plot, you can observe that, marginally, $X_1$ and $X_2$ appear to be correlated; however, correlations are lost once conditioned on *Sex*. \n",
    "\n",
    "<img src=\"./img/naive_bayes/cond_indep_continuous.png\" width=\"650\" height=\"50\"/>\n",
    "\n",
    "\n",
    "**Bayes' Theorem**(*theorem of inverse probability*). Bayes' theorem allows us to find the probability of a *hypothesis*, once an event has been observed . This implies assuming a *prior* probability for the hypothesis. For a hypothesis $H$ and an observed event $E$, Bayes' theorem allows finding $\\Pr(H|E)$: \n",
    "\n",
    "$$\\Pr(H|E) = \\frac{\\Pr(H)\\Pr(E|H)}{\\Pr(E)}$$\n",
    "\n",
    "where: \n",
    "* $\\Pr(H)$ is the **prior** probability\n",
    "* $\\Pr(E|H)$ is the *likelihood*\n",
    "* $\\Pr(E)$ is known as **evidence** or **marginal likelihood**\n",
    "* $\\Pr(H|E)$ is the *posterior probability* of $H$\n",
    "\n",
    "$\\Pr(E)$ is called marginal likelihood because it is the probability of the event, marginalized over all possible hypotheses:\n",
    "\n",
    "<br>\n",
    "$$\\Pr(E) = \\Pr(H)\\Pr(E|H) + \\Pr(H^C)\\Pr(E|H^C).$$\n",
    "<br> \n",
    "\n",
    "For example, if $H$ is composed of 3 events, we would have: \n",
    "\n",
    "<br>\n",
    "$$\\Pr(E) = \\Pr(H=1)\\Pr(E|H=1) + \\Pr(H=2)\\Pr(E|H=2) + \\Pr(H=3)\\Pr(E|H=3).$$\n",
    "<br>\n",
    "\n",
    "For random variables, we can give a similar definition. In particular, given that the *likelihood* is given by $p \\geq 1$ r.v.'s, we can state Bayes' theorem as: \n",
    "\n",
    "<br>\n",
    "$$\\Pr(H|X_1,...,X_p) = \\frac{\\Pr(H)\\Pr(X_1,...,X_p|H)}{\\Pr(H)\\Pr(X_1,...,X_p|H)+\\Pr(H^C)\\Pr(X_1,...,X_p|H^C)} = \\frac{\\Pr(H)\\Pr(X_1,...,X_p|H)}{\\Pr(X_1,...,X_p)}$$\n",
    "<br>\n",
    "\n",
    "In case of continuous $X_1,...,X_p$, we can replace the value $\\Pr(X_1,...,X_p|H)$ with the corresponding value of the multivariate density. \n",
    "\n",
    "Consider the following example. We have two urns, Urn 1 ($H=1$) with 4 blue ($E=B$) and 1 green ($E=G$) balls. And Urn 2 ($H=2$) with 2 blue and 3 green balls. We draw from one of the two urns a green ball. What is the probability that the ball was drawn from Urn 1? And what from Urn 2? \n",
    "\n",
    "<img src=\"./img/naive_bayes/urn_ball.png\" width=\"350\" height=\"50\"/>\n",
    "\n",
    "We have that: \n",
    "\n",
    "* $\\Pr(H=1) = \\Pr(H=2) = 1/2$ as the two urns contain the same number of balls\n",
    "* $\\Pr(E=B|H=1) = 4/5$, while $\\Pr(E=B|H=2) = 2/5$\n",
    "* $\\Pr(E=G|H=1) = 1/5$, while $\\Pr(E=G|H=2) = 3/5$\n",
    "\n",
    "Therefore, by Bayes's theorem, the *posterior probability* of Urn 1 given that we have observed a green ball is: \n",
    "\n",
    "$$\\Pr(H=1|E=G) = \\frac{\\Pr(H=1)\\Pr(E=G|H=1)}{\\Pr(E=G)}$$\n",
    "\n",
    "which is calculated as: \n",
    "\n",
    "$$\\Pr(H=1|E=G) = \\frac{1/2\\cdot1/5}{1/2\\cdot1/5 + 1/2\\cdot3/5} = \\frac{1/10}{4/10} = \\frac{1}{4}$$\n",
    "\n",
    "On the other hand, the posterior probability of drawing a green ball from Urn 2 is:\n",
    "\n",
    "$$\\Pr(H=2|E=G) = \\frac{\\Pr(H=2)\\Pr(E=G|H=2)}{\\Pr(E=G)} = \\frac{1/2\\cdot3/5}{4/10} = \\frac{3}{4} $$\n",
    "\n",
    "The *posterior odd-ratios* are: \n",
    "\n",
    "$$\\frac{\\Pr(H=1|E=G)}{\\Pr(H=2|E=G)} = \\frac{1/4}{3/4} = 1/3 $$\n",
    "\n",
    "which is 3-to-1 in favor of urn 2. (If this ratio was equal to 1, or equivalently the two posterior porbabilities equal to 0.5, then the two urns would have the same probabilities.)\n",
    "\n",
    "## 2. Naive Bayes\n",
    "\n",
    "**Generative vs. Discriminative Classifiers**. \n",
    "\n",
    "**Discriminative models** seek to learn the **decision boundaries** between the classes $Y$, for given values of the features $\\mathbf{X}$. Probabilistically, they try to learn $\\Pr(Y|\\mathbf{X})$. *Logistic regression* is an example of discriminative classifier. \n",
    "\n",
    "**Generative models** seek to learn the **joint distribution** of the features and the classes, trying to model explicitly the distribution of the features within each class. In practice, they try to learn $\\Pr(Y,\\mathbf{X})$; usually this happens by estimating $\\Pr(Y)$ and the conditional $\\Pr(\\mathbf{X}|Y)$, and then finding $\\Pr(Y|\\mathbf{X})$ with Bayes' theorem. *Naive Bayes* is an example of Generative Classifier. \n",
    "\n",
    "$$\\Pr(Y,\\mathbf{X})=\\Pr(Y) \\cdot \\Pr(\\mathbf{X}|Y) $$\n",
    "\n",
    "\n",
    "$$\\Pr(Y|\\mathbf{X})= \\frac{\\Pr(Y) \\cdot \\Pr(\\mathbf{X}|Y)}{\\Pr(\\mathbf{X})} $$\n",
    "\n",
    "**Naive Bayes**. To learn the $\\Pr(\\mathbf{X}|Y)$, generative models need to make some **assumption** to simplify calculations. Naive Bayes makes the following assumption: \n",
    "\n",
    "$$\\Pr(X_1,X_2...,X_p|Y) = \\Pr(X_1|Y)\\Pr(X_2|Y)\\cdots\\Pr(X_p|Y)$$\n",
    "\n",
    "which means the features are assumed to be **conditionally independent**, given the class: \n",
    "\n",
    "$$X_1{\\perp\\!\\!\\!\\perp}X_2{\\perp\\!\\!\\!\\perp}...{\\perp\\!\\!\\!\\perp}X_p|Y$$ \n",
    "\n",
    "This assumption is **very strong** and does not hold usually; however, Naive Bayes can be very **efficient** in high dimensions, as it only needs to consider the univariate distributions of the features. \n",
    "\n",
    "Furthermore, predictions provided by Naive Bayes often work well in practice, since the **posterior probabilities** provided by this algorithm do not need to be **precise**, but rather **assign** data points to the **correct side** of the decision boundaries. \n",
    "\n",
    "**Model Form**. Naive Bayes seeks to find the posterior probability $\\Pr(Y|\\mathbf{X})$ with the aid of Bayes' theorem. In particular, for $Y \\in \\{1,...,C\\}$ the posterior probability of class $c$ is calculated as follows: \n",
    "\n",
    "<br>\n",
    "$$\\Pr(Y=c|X_1,...,X_p) = \\frac{\\Pr(Y=c)\\Pr(X_1,...,X_p|Y=c)}{\\Pr(X_1,...X_p)} = \\frac{\\Pr(Y=c)\\Pr(X_1|Y=c)\\cdots\\Pr(X_P|Y=c)}{\n",
    "\\sum_{k=1}^{C} \\Pr(Y=k)\\Pr(X_1|Y=k)\\cdots\\Pr(X_p|Y=k)\n",
    "}$$\n",
    "<br>\n",
    "\n",
    "Such probabilities are estimated from the data (as explained in the next sections).  \n",
    "\n",
    "After the joint probabilities $\\Pr(Y,\\mathbf{X})$ have been computed (model training), predictions occur by assigning the units to the class $c$ such that \n",
    "\n",
    "$\\Pr(Y=c|X_1,...,X_p)$\n",
    "\n",
    "is **maximized** (or **equivalently** the class $c$ that maximizes $\\Pr(Y=c)\\Pr(X_1,...,X_p|Y=c) = \\Pr(Y=c,\\mathbf{X})$, since the marginal likelihood is the same for all classes). \n",
    "\n",
    "The **decision boundary** is given by all those point of $\\mathbf{x}$ in which the classes have equal posterior probabilities. \n",
    "\n",
    "Depending on the **nature** of the **features** and on the **assumptions** made on the univariate distributions $\\Pr(X_j|Y)_{j=1,...,p}$, we can have different types of Naive Bayes models. In particular: \n",
    "\n",
    "* we can use *categorical Naive Bayes* in presence of categorical data\n",
    "* we can use *Multinomial Naive Bayes* in presence of count data, and Multinomial distributions are assumed\n",
    "* we can use *Gaussian Naive Bayes* in presence of continuous data, and Gaussian distributions are assumed\n",
    "\n",
    "In what follows, we are going to see the specific form of such models, as well as some examples. \n",
    "\n",
    "\n",
    "### 2.1 Categorical Naive Bayes \n",
    "Consider the *Contact Lenses Dataset*, reporting what type of contact lenses (the classes) an ophthalmologist should prescribe (n=*none*, s=*soft*, h=*hard*), given four nominal features: age (y=*young*, pp=*pre-presbyopic*, pr=*presbyopic*), spectacle prescription (m=*myope*, h=*hypermetrope*), astigmatism (y=*yes*, n=*no*), rate of tear production (r=*reduced*, n=*normal*). For this example, we consider only few observations of the dataset (in the Python examples, you will see the whole dataset): \n",
    "\n",
    "|ID| $X_1$: Age | $X_2$: Spectacle-Prescription |  $X_3$: Astigmatism | $X_4$: Tear-Prod Rate | $Y$: Contact Lenses|  \n",
    "|:----:|:------:|:-----:|:-----:|:-----:|:-----:|\n",
    "|1|y | m | n | r | n | \n",
    "|2|y| m | y| n | s | \n",
    "|3|pp | m | n | r | n | \n",
    "|4|pp | m | y | n | h | \n",
    "|5|pp | m | n | n | s | \n",
    "|6|pr | m | y | n | h | \n",
    "|7|y | h | n | r | n | \n",
    "|8| y | h | n | n | s | \n",
    "|9| y | h | y | n | h | \n",
    "|10| pp | h | n | n | s | \n",
    "|11| pr | h | n | n | s | \n",
    "|12|pr | h | y | r | n | \n",
    "\n",
    "\n",
    "**Training**. In presence of only **nominal features**, we can compute the **class-specific** counts of the categories of each variable, and subsequently turn such counts into probabilities. This is what **Categorical Naive Bayes** does.\n",
    "\n",
    "In practice, it is assumed that each feature $X_j$ ($j=1,...,p$) follows a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution), and the $j$-th feature can take on $S_j \\geq 2$ categories (in the case above, for example, $X_1$ has $S_1=3$ categories: $s \\in \\{y, pp, pr\\}$).\n",
    "\n",
    "For the $s$-th category of $X_j$ , we estimate the probability: \n",
    "\n",
    "<br>\n",
    "$$\\Pr(X_j=s|Y=c) = \\frac{\\sum_{i:y_i=c}\\mathcal{I}(x_{ij}=s)}{\\sum_{i=1}^{n} \\mathcal{I}(y_i=c)} \\ \\forall\\ c, j, s$$\n",
    "<br>\n",
    "\n",
    "and $\\mathcal{I}(a)$ is the indicator function, equal to $1$ when $a$ is True, and 0 otherwise. In words, this means that we perform this operation:\n",
    "\n",
    "\n",
    "<br>\n",
    "$$\\Pr(X_j=s|Y=c) = \\frac{\\#\\ of\\ times\\ category\\ s\\ of\\ variable\\ X_j\\ is\\ observed\\ in\\ the\\ c-th\\ class}\n",
    "{\\#\\ of\\ observations\\ in\\ class\\ c}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "With $p=2$ variables, the posterior probability of class $c$ given that $X_1 = s_1$ and $X_2 = s_2$ is observed, we would compute: \n",
    "\n",
    "<br>\n",
    "$$\\Pr(Y=c|X_1=s_1,X_2=s_2) = \\frac{\\Pr(Y=c)\\Pr(X_1=s_1|Y=c)\\Pr(X_2=s_2|Y=c)}{\\sum_{k=1}^{C}Pr(Y=k)\\Pr(X_1=s_1|Y=k)\\Pr(X_2=s_2|Y=k)}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "\n",
    "Let's now manually train the model on the *contact lenses* data, by computing class counts and class-specific feature counts. First, let's start from the *prior probabilities* of $Y$: \n",
    "\n",
    "| |$Y$=n|$Y$=s|$Y$=h|Total|\n",
    "|:------:|:------:|:------:|:------:|:------:|\n",
    "|Count | 4 | 5 | 3 | 12 | \n",
    "|Pr(Y) | 4/12 | 5/12 | 3/12 | 1 | \n",
    "    \n",
    "\n",
    "Now the counts for the class $Y=n$: \n",
    "\n",
    "\n",
    "|Counts, Y=n| $X_1$ | # | $X_2$ | # |   $X_3$ | #  | $X_4$ | # |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   2   |  $X_2$=m |  2   | $X_3$=n |   3  | $X_4$=r |  4    |\n",
    "||  $X_1$=pp   |   1   |  $X_2$=h |  2   | $X_3$=y |   1  | $X_4$=n |  0    |\n",
    "||  $X_1$=pr   |   1   |    |     |   |     |   |      |\n",
    "\n",
    "<br>\n",
    "that leads to the following estimated probabilities: \n",
    "\n",
    "|Pr, Y=n| $X_1$ | Pr | $X_2$ | Pr |   $X_3$ | Pr  | $X_4$ | Pr |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |  1/2    |  $X_2$=m |  1/2   | $X_3$=n |  3/4   | $X_4$=r |  1    |\n",
    "||  $X_1$=pp   |   1/4   |  $X_2$=h |  1/2   | $X_3$=y |  1/4   | $X_4$=n |  0    |\n",
    "||  $X_1$=pr   |   1/4   |    |     |   |     |   |      |\n",
    "\n",
    "\n",
    "The counts for class $Y=s$: \n",
    "\n",
    "\n",
    "|Counts, Y=s| $X_1$ | # | $X_2$ | # |   $X_3$ | #  | $X_4$ | # |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |  2    |  $X_2$=m |  2   | $X_3$=n |  4   | $X_4$=r |   0   |\n",
    "||  $X_1$=pp   |  2    |  $X_2$=h |  3   | $X_3$=y |  1   | $X_4$=n |   5   |\n",
    "||  $X_1$=pr   |  1    |    |     |   |     |   |      |\n",
    "\n",
    "<br>\n",
    "with corresponding probabilities: \n",
    "\n",
    "|Pr, Y=s| $X_1$ | Pr | $X_2$ | Pr |   $X_3$ | Pr  | $X_4$ | Pr |   \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   2/5   |  $X_2$=m |  2/5   | $X_3$=n |  4/5   | $X_4$=r |  0    |\n",
    "||  $X_1$=pp   |   2/5   |  $X_2$=h |  3/5   | $X_3$=y |  1/5   | $X_4$=n |   1   |\n",
    "||  $X_1$=pr   |   1/5   |    |     |   |     |   |      |\n",
    "\n",
    "\n",
    "\n",
    "Last, the counts for class $Y=h$: \n",
    "\n",
    "|Counts, Y=h| $X_1$ | # | $X_2$ | # |   $X_3$ | #  | $X_4$ | # |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   1   |  $X_2$=m |  3   | $X_3$=n |  0   | $X_4$=r |   0   |\n",
    "||  $X_1$=pp   |   1   |  $X_2$=h |  0   | $X_3$=y |  3   | $X_4$=n |   3   |\n",
    "||  $X_1$=pr   |   1   |    |     |     |     |   |      |\n",
    "\n",
    "<br>\n",
    "with estimated probabilities:\n",
    "\n",
    "|Pr, Y=h| $X_1$ | Pr | $X_2$ | Pr |   $X_3$ | Pr  | $X_4$ | Pr |   \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   1/3   |  $X_2$=m |  1   | $X_3$=n |  0   | $X_4$=r |  0    |\n",
    "||  $X_1$=pp   |   1/3   |  $X_2$=h |  0   | $X_3$=y |  1   | $X_4$=n |  1    |\n",
    "||  $X_1$=pr   |   1/3   |    |     |   |     |   |      |\n",
    "\n",
    "\n",
    "<br>\n",
    "Done! Our Naive Bayes classifier is trained. According to the estimated probabilities, $X_2$ shouldn't be too discriminant for class $n$ (*none* contact lenses), but it could be highly discriminant for class $h$ (*hard* contact lenses). On the other hand, feature $X_4$ seems to be highly discriminant for all four classes. (Why is that? Try to give your own explanation, and see if you can detect other highly discriminant features for each class). \n",
    "\n",
    "\n",
    "**Note**. With the **Naive Bayes** algorithm we can directly work with **multiple classes ($C>2$)** (in this example we have three classes). \n",
    "\n",
    "In practice, libraries like **scikit-learn** still require **encoded features**, as we will see in the examples section.  \n",
    "\n",
    "\n",
    "\n",
    "**Prediction**. Now, let's suppose we have a new observation $\\mathbf{x}^*$ with the following characteristics: \n",
    "\n",
    "$$\\mathbf{x}^* = \\left(X_1=y, X_2=m, X_3=y, X_4=n\\right) $$\n",
    "\n",
    "How do we classify $\\mathbf{x}^*$? Let's compute $\\Pr(Y,\\mathbf{x}^*)$ given $\\mathbf{x}^*$.  \n",
    "\n",
    "\n",
    "$$\\Pr(Y=n, \\mathbf{x}^*) = \\frac{4}{12}\\cdot\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{1}{4}\\cdot0 = 0$$\n",
    "\n",
    "$$\\Pr(Y=s, \\mathbf{x}^*) = \\frac{5}{12}\\cdot\\frac{2}{5}\\cdot\\frac{2}{5}\\cdot\\frac{1}{5}\\cdot1 \\approx 0.013$$\n",
    "\n",
    "$$\\Pr(Y=h, \\mathbf{x}^*) = \\frac{3}{12}\\cdot\\frac{1}{3}\\cdot1\\cdot1\\cdot1 \\approx 0.083 $$\n",
    "\n",
    "Thus, the unit has a larger chance to be classified in the *hard contact lenses* class. We can also compute the posterior probabilities:\n",
    "\n",
    "<br>\n",
    "$$\\Pr(Y=n|\\mathbf{x}^*) = 0 $$\n",
    "<br>\n",
    "$$\\Pr(Y=s|\\mathbf{x}^*) = \\frac{0.013}{0+0.013+0.083} \\approx 0.135 $$\n",
    "<br>\n",
    "$$\\Pr(Y=h|\\mathbf{x}^*) = \\frac{0.083}{0+0.013+0.083} \\approx 0.865 $$\n",
    "<br>\n",
    "\n",
    "The new observation will then be classified into the *hard lenses* class. \n",
    "\n",
    "\n",
    "\n",
    "### 2.2 Count Smoothing\n",
    "The posterior probability of class $n$ (**none contact lenses**) in this last example is equal to **0**. This is due to the fact that, within the class ***none contact lenses***, there was **no** observation having **Tear production rate=normal**.\n",
    "\n",
    "It seems quite **implausible** that in the whole population there won't be any patient having normal tear production rate, without contact lenses prescription ($Y=n$). In this case **Naive Bayes** is trained with probabilities that are too **sample-specific** (and therefore it is led to **overfit**). \n",
    "\n",
    "Note that the same problem may arise when we had to classify an observation having **reduced** tear production rate in the **soft** and **hard** contact lenses classes, or with *no astigmatism* in the same classes.  \n",
    "\n",
    "To **overcome** the problem, there is a simple trick to **avoid zero-counts** in the features count tables: adding a number of **imaginary counts**, $\\alpha > 0$, to each cell of the table. This will **smooth** the resulting probabilities, **avoiding degenerate solutions** as in the case above. For this reason, such operation is called **smoothing**. \n",
    "\n",
    "The new feature-specific probabilities are then computed as follows (remember that $S_j$ is the number of categories of feature $X_j$):\n",
    "\n",
    "<br>\n",
    "$$\\Pr(X_j=s|Y=c) = \\frac{\\sum_{i:y_i=c}\\mathcal{I}(x_{ij}=s) + \\alpha}{\\sum_{i=1}^{n} \\mathcal{I}(y_i=c) + S_j\\alpha}\\ \\forall\\ c, j, s.$$\n",
    "<br>\n",
    "\n",
    "When $\\alpha=1$, this is called **Laplace smoothing**. \n",
    "\n",
    "The solutions provided by **Naive Bayes** are in general **quite robust** to the **choice of $\\alpha$**, but this can be seen as a **tunable** hyperparameter used to avoid overfitting. \n",
    "\n",
    "Keep in mind that the **largest** the value of $\\alpha$, the more the **conditional distributions** of the features will tend to **uniformity**, reducing their discriminative power (leading to more equi-probably classes). \n",
    "\n",
    "Let's reconsider the count tables in the *contact lenses* example, this time introducing the Laplace smoothing: \n",
    "\n",
    "**Class $Y=n$**. \n",
    "<br> \n",
    "\n",
    "|Counts, Y=n| $X_1$ | # | $X_2$ | # |   $X_3$ | #  | $X_4$ | # |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   3   |  $X_2$=m |  3    | $X_3$=n |   4  | $X_4$=r |  5 |\n",
    "||  $X_1$=pp   |   2   |  $X_2$=h |  3    | $X_3$=y |   2  | $X_4$=n |  1 |\n",
    "||  $X_1$=pr   |   2   |    |     |  |     |   |        |\n",
    "\n",
    "<br>\n",
    "\n",
    "|Pr, Y=n| $X_1$ | Pr | $X_2$ | Pr |   $X_3$ | Pr  | $X_4$ | Pr |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |  3/7    |  $X_2$=m |  1/2   | $X_3$=n |  3/4   | $X_4$=r |  5/6    |\n",
    "||  $X_1$=pp   |   2/7   |  $X_2$=h |  1/2   | $X_3$=y |  1/4   | $X_4$=n |  1/6    |\n",
    "||  $X_1$=pr   |   2/7   |    |     |   |     |   |      |\n",
    "\n",
    "\n",
    "**Class $Y=s$**. \n",
    "\n",
    "\n",
    "|Counts, Y=s| $X_1$ | # | $X_2$ | # |   $X_3$ | #  | $X_4$ | # | \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |  3    |  $X_2$=m |  3   | $X_3$=n |  5   | $X_4$=r |   1   |\n",
    "||  $X_1$=pp   |  3    |  $X_2$=h |  4   | $X_3$=y |  2   | $X_4$=n |   6   |\n",
    "||  $X_1$=pr   |  2    |    |     |   |     |   |      |\n",
    "\n",
    "<br>\n",
    "with corresponding probabilities: \n",
    "\n",
    "|Pr, Y=s| $X_1$ | Pr | $X_2$ | Pr |   $X_3$ | Pr  | $X_4$ | Pr |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   3/8   |  $X_2$=m |  3/7   | $X_3$=n |  5/7   | $X_4$=r |  1/7    |\n",
    "||  $X_1$=pp   |   3/8   |  $X_2$=h |  4/7   | $X_3$=y |  2/7   | $X_4$=n |  6/7   |\n",
    "||  $X_1$=pr   |   1/4   |    |     |   |     |   |      |\n",
    "\n",
    "\n",
    "\n",
    "**Class $Y=h$**.\n",
    "\n",
    "|Counts, Y=h| $X_1$ | # | $X_2$ | # |   $X_3$ | #  | $X_4$ | # |   \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   2   |  $X_2$=m |  4   | $X_3$=n |  1   | $X_4$=r |   1   |\n",
    "||  $X_1$=pp   |   2   |  $X_2$=h |  1   | $X_3$=y |  4   | $X_4$=n |   4   |\n",
    "||  $X_1$=pr   |   2   |    |     |     |     |   |      |\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "|Pr, Y=h| $X_1$ | Pr | $X_2$ | Pr |   $X_3$ | Pr  | $X_4$ | Pr |  \n",
    "|:------:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "||  $X_1$=y    |   1/3   |  $X_2$=m |  4/5   | $X_3$=n |  1/5   | $X_4$=r |  1/5    |\n",
    "||  $X_1$=pp   |   1/3   |  $X_2$=h |  1/5   | $X_3$=y |  4/5   | $X_4$=n |  4/5    |\n",
    "||  $X_1$=pr   |   1/3   |    |     |   |     |   |      |\n",
    "\n",
    "\n",
    "We now avoid probabilities that are exact zero and perform a \"softer\" classification task. You can also notice that the imaginary count have less effect on the distributions of classes having more true observations. Let's now review the example with the vector of observations introduced in the last section, $x^*$: \n",
    "\n",
    "\n",
    "$$\\mathbf{x}^* = \\left(X_1=y, X_2=m, X_3=y, X_4=n\\right) $$\n",
    "\n",
    "The new probabilities are: \n",
    "<br>\n",
    "$$\\Pr(Y=n, \\mathbf{x}^*) = \\frac{4}{12}\\cdot\\frac{3}{7}\\cdot\\frac{1}{2}\\cdot\\frac{1}{4}\\cdot\\frac{1}{6} \\approx 0.003 $$\n",
    "<br>\n",
    "$$\\Pr(Y=s, \\mathbf{x}^*) = \\frac{5}{12}\\cdot\\frac{3}{8}\\cdot\\frac{3}{7}\\cdot\\frac{2}{7}\\cdot\\frac{6}{7} \\approx 0.016 $$\n",
    "<br>\n",
    "$$\\Pr(Y=h, \\mathbf{x}^*) = \\frac{3}{12}\\cdot\\frac{1}{3}\\cdot\\frac{4}{5}\\cdot\\frac{4}{5}\\cdot\\frac{4}{5} \\approx 0.043  $$\n",
    "<br>\n",
    "With the following posterior probabilities: \n",
    "\n",
    "\n",
    "<br>\n",
    "$$\\Pr(Y=n|\\mathbf{x}^*) = \\frac{0.003}{0.003+0.016+0.043} \\approx 0.05 $$\n",
    "<br>\n",
    "$$\\Pr(Y=s|\\mathbf{x}^*) = \\frac{0.016}{0.003+0.016+0.043} \\approx 0.26 $$\n",
    "<br>\n",
    "$$\\Pr(Y=h|\\mathbf{x}^*) = \\frac{0.043}{0.003+0.016+0.043} \\approx 0.69 $$\n",
    "<br>\n",
    "\n",
    "$\\mathbf{x}^*$ is classified in the same class of the non-smoothed case (*hard contact lenses*); the probability of *hard lenses*, however, is not as dominating as before now: the probability to be in the *soft contact lenses* class has increased, and the *none contact lenses class* doesn't have probability 0 anymore. \n",
    "\n",
    "<br> \n",
    "\n",
    "As an *exercise*, try to perform smoothing and compute the new posterior probabilities when $\\alpha=0.5$ and $\\alpha=2$, and see how they change. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Examples in Python\n",
    "Here we are going to use the algorithms contained in the `sklearn.naive_bayes` module. \n",
    "\n",
    "To **ease** the exposition, we are going simply to perform **model initialization** and **output exploration**, and therefore we **are not going to perform split** of the data into test and training set. \n",
    "\n",
    "This step, of course, must be performed in real applications. As **an exersise** do all the mentioned process below, considering split of the data into test and training set. \n",
    "\n",
    "###  Categorical Naive Bayes\n",
    "Categorical Naive Bayes can be trained and used for predictions with the [CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) function. The main hyperparameter is `alpha`, which is the **imaginary count** for each cell of the contingency table (introduced above). \n",
    "\n",
    "For this example, we are going to use the **full version** of the **contact lenses**  dataset introduced in section 2.1, which contains 24 instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset \n",
    "contact_lenses = pd.read_csv(\"./data/naive_bayes/contact_lenses.csv\")\n",
    "contact_lenses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>spectacle-prescrip</th>\n",
       "      <th>astigmatism</th>\n",
       "      <th>tear-prod-rate</th>\n",
       "      <th>contact-lenses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>no</td>\n",
       "      <td>normal</td>\n",
       "      <td>soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>reduced</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>young</td>\n",
       "      <td>myope</td>\n",
       "      <td>yes</td>\n",
       "      <td>normal</td>\n",
       "      <td>hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>young</td>\n",
       "      <td>hypermetrope</td>\n",
       "      <td>no</td>\n",
       "      <td>reduced</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age spectacle-prescrip astigmatism tear-prod-rate contact-lenses\n",
       "0  young              myope          no        reduced           none\n",
       "1  young              myope          no         normal           soft\n",
       "2  young              myope         yes        reduced           none\n",
       "3  young              myope         yes         normal           hard\n",
       "4  young       hypermetrope          no        reduced           none"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contact_lenses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = contact_lenses.drop(\"contact-lenses\", axis=1)\n",
    "y = contact_lenses[\"contact-lenses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CategoricalNB` expects the categorical features to be encoded into integers. For this purpose, we use the ordinal encoder transformer of scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 1., 0., 1.],\n",
       "       [2., 1., 0., 0.],\n",
       "       [2., 1., 1., 1.],\n",
       "       [2., 1., 1., 0.],\n",
       "       [2., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_enc = OrdinalEncoder()\n",
    "X_enc = ord_enc.fit_transform(X)\n",
    "X_enc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories have been encoded in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['pre-presbyopic', 'presbyopic', 'young'], dtype=object),\n",
       " array(['hypermetrope', 'myope'], dtype=object),\n",
       " array(['no', 'yes'], dtype=object),\n",
       " array(['normal', 'reduced'], dtype=object)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_enc.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for the feature `age`, the value 'young' was encoded with the integer values 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train the Naive Bayes algorithm with **Laplace smoothing**,  $\\alpha=1$ which is  the **default** option in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalNB(alpha=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing and training CategoricalNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "cnb = CategoricalNB(alpha=1)\n",
    "cnb.fit(X_enc,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **class-specific counts** can be retrieved with the `category_count_` attribute. For the first feature, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 2.],\n",
       "       [5., 6., 4.],\n",
       "       [2., 1., 2.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.category_count_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each **row** corresponds to a **class**, and each **column** to a **category of the feature**. For the last feature (tear-prod-rate) we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  0.],\n",
       "       [ 3., 12.],\n",
       "       [ 5.,  0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.category_count_[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class count, instead, can be retrieved via: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 15.,  5.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict_proba` method returns the predicted probabilities. Let's inspect them for the first five units in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03798492, 0.8442812 , 0.11773388],\n",
       "       [0.16427948, 0.22470143, 0.61101909],\n",
       "       [0.16381803, 0.81925689, 0.01692508],\n",
       "       [0.69845351, 0.21495251, 0.08659398],\n",
       "       [0.01687038, 0.84369075, 0.13943887]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.predict_proba(X_enc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with corresponding predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none', 'soft', 'none', 'hard', 'none'], dtype='<U4')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.predict(X_enc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================================================================================================\n",
    "\n",
    "## Extra topics for interested students: \n",
    "\n",
    "\n",
    "==========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multinomial Naive Bayes \n",
    "One of the fields in which Naive Bayes is most successful is document classification. The goal is to classify documents by given topics, using the words that appear in each document to try to identify the different classes. \n",
    "\n",
    "\n",
    "<img src=\"./img/naive_bayes/doc_classification.png\" width=\"750\" height=\"50\"/>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "In this case, preserving the information regarding the frequency of the *words* of a pre-built vocabulary that appear in each document is relevant. With categorical Naive Bayes, information about such type of counts is not taken into account. Instead, by assuming a *multinomial* distribution for the features instead of a categorical one, Naive Bayes can accommodate for this new information. \n",
    "\n",
    "Each feature refers now to the words in the vocaulary (so that we have words $j = 1,...,p$ in the vocabulary), and the various instances are the documents in the collection. The values taken on by the features are the number of times that each word $j$ appears in document $i$. Since we are dealing with counts, and we are usually dealing with extremely high dimensional datasets (often thousands of words), it is not rare to having to deal with *sparse datasets* (i.e., datasets having a lot of zeros). We can then use count smoothing to avoid documents with zero-probabilities within some of the classes. \n",
    "\n",
    "**Training**. The probability of a document $\\mathbf{x} = (x_{1},...,x_{p})$ (where each $x_j$ is the number of times that word $j$ appears in $\\mathbf{X}$, and $n_x =\\sum_{j=1}^{p}x_{j}$ is the length of the document) conditional on class $c$ is ruled by the multinomial distribution: \n",
    "\n",
    "<br>\n",
    "$$\\Pr(\\mathbf{x}|Y=c) = {n_x\\choose{x_{1}\\cdots x_{p}}}\\prod_{j=1}^{p}\\pi_{jc}^{x_j}$$\n",
    "<br>\n",
    "\n",
    "The first term on the right-hand side of the equation is the *multinomial coefficient* (which allows for any order of the words in the document), while $\\pi_{jc}$ is the word-specific conditional probability, computed with (including $\\alpha$ imaginary counts):\n",
    "\n",
    "\n",
    "<br>\n",
    "$$\\pi_{jc} = \\frac{\\sum_{i:y_i=c}\\mathcal{I}(x_{ij}) + \\alpha}{\\sum_{i:y_i=c} n_{x_i} + p\\alpha}$$\n",
    "<br>\n",
    "\n",
    "where $n_{x_i} = \\sum_{j=1}^{p}x_{ij}$ is the length of document $i$. In words, the probability can be expressed as (without imaginary counts): \n",
    "\n",
    "<br>\n",
    "$$\\pi_{jc}= \\frac{\\#\\ of\\ times\\ word\\ j\\ appears\\ in\\ class\\ c}{\\#\\ of\\ words\\ of\\ all\\ documents\\ of\\ class\\ c}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "Once such probabilities are computed, prediction can be performed with the Bayes' theorem, as done in the categorical case. In section 3, we will see an example of application of Multinomial Naive Bayes with Python. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multinomial Naive Bayes in Python\n",
    "Multinomial Naive Bayes can be implemented with [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Similarly to CategoricalNB, the main hyperparameter is $\\alpha$, for count smoothing. \n",
    "\n",
    "For this small example, we will use a pre-processed (and reduced) version of the [SMS Spam Collection dataset](https://www.kaggle.com/ishansoni/sms-spam-collection-dataset), which collects SMS text data and labels them as ham or spam. In the dataset of this example, a vocabulary was built starting from the original messages, so that each column corresponds to a word of the vocabulary, and each row to a different message. (Note that a word can also be a single character). The feature values measure how many times the corresponding word appears in the message. In this reduced version, there are $n=218$ messages and $p=25$ words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset \n",
    "ham_spam = pd.read_csv(\"./data/naive_bayes/ham_spam.csv\")\n",
    "ham_spam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ham_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ham_spam.drop(\"class\", axis=1)\n",
    "y = ham_spam[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data matrix is *sparse*. Let's now check the words in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a Multinomial Naive Bayes classifier, trying to detect which words are more common in 'ham' messages, and which ones are more common in 'spam' messages (as done earlier, we use Laplace smoothing): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are sorted by MultinomianNB in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class-specific word counts are the following ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[X.columns, mnb.feature_count_[0], mnb.feature_count_[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that characters like 'U' and 'u', and terms like 'ltgt' appear much more frequently in ham than spam, while words such as 'FREE' and 'week' appear more frequently in spam messages. \n",
    "\n",
    "We now ask for the posterior class probabilities for the first five training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.predict_proba(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and their predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.predict(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Gaussian Naive Bayes\n",
    "When the features are continuous, it is possible to assume Gaussian distributions. For example, with two continuous features $X_1$ and $X_2$:\n",
    "\n",
    "\n",
    "<img src=\"./img/naive_bayes/naive_bayes_gaussian.png\" width=\"400\" height=\"50\"/>\n",
    "\n",
    "\n",
    "\n",
    "The more the true distribution of (some of) the features deviates from the Gaussian, the less the Naive Bayes classifier becomes accurate. \n",
    "\n",
    "**Training**. In this case, the feature-specific conditional distribution is a Gaussian, with mean and variance estimated from the data (means and variances are estimated for each class). Once means and variances have been calculated, the conditional probability of feature $X_j$ can be obtained by plugging their values into the formula of the Normal distribution, conditioned on class $c$: \n",
    "\n",
    "$$f(X_j=x_j|Y=c; \\mu_{jc}, \\sigma^2_{jc}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_{jc}}}e^{-\\frac{1}{2}\\left(\\frac{x_j-\\mu_{jc}}{\\sigma_{jc}}\\right)^2} $$\n",
    "\n",
    "where $\\mu_{jc}$ is the estimated class-specific mean of feature $X_j$: \n",
    "\n",
    "$$\\mu_{jc} = \\frac{\\sum_{i:y_i=c} x_{ij}}{\\sum_{i=1}^{n} \\mathcal{I}(y_i=c)}$$\n",
    "\n",
    "\n",
    "and $\\sigma^2_{jc}$ is the estimated class-specific variance of feature $X_j$: \n",
    "\n",
    "$$\\sigma^2_{jc} = \\frac{\\sum_{i:y_i=c} (x_{ij}-\\mu_{jc})^2}{\\sum_{i=1}^{n} \\mathcal{I}(y_i=c) -1}$$\n",
    "\n",
    "After calculating $f(X_j=x_j|Y=c; \\mu_{jc}, \\sigma^2_{jc})$ for each $j$, Bayes' theorem can be used to predict new data,as done for the categorical and multinomial case. This version of Naive Bayes, unlike the previous ones, does not have any regularization parameter in its standard form. \n",
    "\n",
    "**Example**. Consider the [Iris Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). The goal is to classify iris flowers into three possible families ($Y$): *iris-setosa* ($=se$), *iris-virginica* ($=vi$), and *iris-versicolor* ($=ve$) from four characteristics, *sepal length*, *sepal width*, *petal length*, and *petal width*. Here we consider a reduced version of the dataset, containing only features $X_1$:*petal length* and $X_2$: *petal width*. Furthermore, for this small example we only use a subset of $n=12$ flowers.\n",
    "\n",
    "\n",
    "\n",
    "|ID| $X_1$ | $X_2$  | $Y$|  \n",
    "|:----:|:------:|:-----:|:-----:|\n",
    "|1| 1.4 | 0.2 | se | \n",
    "|2| 1.3 | 0.2 | se | \n",
    "|3| 1.7 | 0.3 | se | \n",
    "|4| 1.4 | 0.3 | se | \n",
    "|5| 4.7 | 1.4 | ve | \n",
    "|6| 4.9 | 1.5 | ve | \n",
    "|7| 3.3 | 1.0 | ve | \n",
    "|8| 3.0 | 1.3 | ve | \n",
    "|9| 6.0 | 2.5 | vi | \n",
    "|10| 5.1 | 1.9 | vi | \n",
    "|11| 6.6 | 2.1 | vi | \n",
    "|12| 4.5 | 1.7 | vi | \n",
    "\n",
    "\n",
    "The class-specific means and variances of $X_1$ are (check the results by yourself!): \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$Class=se: \\mu_{X_1,se} = 1.45,\\ \\sigma^2_{X_1,se} = 0.03 $$\n",
    "<br>\n",
    "$$Class=ve: \\mu_{X_1,ve} = 3.975,\\ \\sigma^2_{X_1,ve} = 0.93 $$\n",
    "<br>\n",
    "$$Class=vi: \\mu_{X_1,vi} = 5.55,\\ \\sigma^2_{X_1,vi} = 0.87 $$\n",
    "\n",
    "And for $X_2$: \n",
    "<br>\n",
    "$$Class=se: \\mu_{X_2,se} = 0.25,\\ \\sigma^2_{X_2,se} = 0.003 $$\n",
    "<br>\n",
    "$$Class=ve: \\mu_{X_2,ve} = 1.3,\\ \\sigma^2_{X_2,ve} = 0.05 $$\n",
    "<br>\n",
    "$$Class=vi: \\mu_{X_2,vi} = 2.05,\\ \\sigma^2_{X_2,vi} = 0.12 $$\n",
    "\n",
    "\n",
    "\n",
    "We now want to classify an observation $\\mathbf{x}^*=\\left(x_1^*=2, x_2^*=0.4\\right).$ The (joint) class specific conditional densities are: \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$ f(\\mathbf{x}^*|Y=se) = f(x^*_1|Y=se;\\mu_{X_1,se},\\sigma^2_{X_1,se})f(x^*_2|Y=se;\\mu_{X_2,se}, \\sigma^2_{X_2,se}) \\approx 0.015\\cdot 0.17 = 0.00255\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "$$ f(\\mathbf{x}^*|Y=ve) = f(x^*1|Y=ve;\\mu_{X_1,ve},\\sigma^2_{X_1,ve})f(x^*_2|Y=ve;\\mu_{X_2,ve}, \\sigma^2_{X_2,ve})  \\approx 0.0035 \\cdot 0.0005 = 1.75\\cdot10^{-6}\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "and finally: \n",
    "$$ f(\\mathbf{x}^*|Y=vi) = f(x^*_1|Y=vi;\\mu_{X_1,vi},\\sigma^2_{X_1,vi})f(x^*_2|Y=vi;\\mu_{X_2,vi}, \\sigma^2_{X_2,vi}) \\approx 2.91\\cdot10^{-6} \\cdot 1.36\\cdot10^{-5} \\approx 0\n",
    "$$\n",
    "\n",
    "(As an exercise, write a function in Python that calculates the density of the normal distribution and verify such calculations. Alternatively, find a Python function that calculates the density of the normal and calculate the densities above. *Tip*: check the *scipy* library). \n",
    "\n",
    "\n",
    "\n",
    "Now, to perform prediction, we can simply multiply these conditional densities by the prior probability of each $Y$, and normalize. In this case, we have $\\Pr(Y)=1/3$ for all classes, and therefore the posterior probabilities become:  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$\\Pr(Y=se|\\mathbf{x}^*) = \\frac{1/3\\cdot0.00255}{1/3\\cdot0.00255+(1/3\\cdot1.75\\cdot10^{-6})+0} = 0.9996$$\n",
    "<br>\n",
    "$$\\Pr(Y=ve|\\mathbf{x}^*) = \\frac{1/3\\cdot1.75\\cdot10^{-6}}{1/3\\cdot0.00255+(1/3\\cdot1.75\\cdot10^{-6})+0} = 0.0004$$\n",
    "<br>\n",
    "$$\\Pr(Y=vi|\\mathbf{x}^*) = 0.$$\n",
    "<br>\n",
    "\n",
    "The new observation is going to be classified in the *iris setosa* class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Gaussian Naive Bayes in Python\n",
    "Gaussian Naive Bayes is implemented with [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). \n",
    "\n",
    "We are going to train it in the *Iris* dataset, using only two features: petal length and petal width. The dataset contains all $n=150$ measurement present in its original version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "iris_red = pd.read_csv(\"./data/naive_bayes/iris_reduced.csv\")\n",
    "iris_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are encoded in the following way: $0=$*setosa*, $1=$*versicolor*, $2=$*virginica*. Let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris_red.iloc[:,:2]\n",
    "y = iris_red['class']\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(X.loc[y==0].iloc[:,0], X.loc[y==0].iloc[:,1], \"bo\", label=\"Setosa\")\n",
    "plt.plot(X.loc[y==1].iloc[:,0], X.loc[y==1].iloc[:,1], \"ro\", label=\"Versicolor\")\n",
    "plt.plot(X.loc[y==2].iloc[:,0], X.loc[y==2].iloc[:,1], \"go\", label=\"Virginica\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train Gaussian Naive Bayes on these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated class-specific means are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.theta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the average *petal_length* for the class *iris setosa* is 1.462, while the average *petal_width* for the class *iris virginica* is 2.062. We can also inspect the variances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.sigma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted posterior probabilities of the classes for the first five training units are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict_proba(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result makes sense, as all the first five instances in the training dataset come from the *iris setosa* family. And their predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, let's plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX1, XX2 = np.meshgrid(np.linspace(X.iloc[:,0].min(), X.iloc[:,0].max(), 30), \n",
    "                       np.linspace(X.iloc[:,1].min(), X.iloc[:,1].max(), 30))\n",
    "ZZ = gnb.predict(np.c_[XX1.ravel(), XX2.ravel()]).reshape(XX1.shape)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(XX1, XX2, ZZ, levels=1, colors='k')\n",
    "plt.plot(X.loc[y==0].iloc[:,0], X.loc[y==0].iloc[:,1], \"bo\", label=\"Setosa\")\n",
    "plt.plot(X.loc[y==1].iloc[:,0], X.loc[y==1].iloc[:,1], \"ro\", label=\"Versicolor\")\n",
    "plt.plot(X.loc[y==2].iloc[:,0], X.loc[y==2].iloc[:,1], \"go\", label=\"Virginica\")\n",
    "plt.xlabel(\"petal_length\")\n",
    "plt.ylabel(\"petal_width\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example with the Heart Dataset \n",
    "The Heart dataset is composed of mixed continuous and categorical features. The Naive Bayes models implemented in scikit-learn do not support such datasets. To estimate Naive Bayes with this dataset, we are going to follow the following strategy: \n",
    "* we will upload the original (non-preprocessed) dataset\n",
    "* we will perform median imputation on the continuous features, and mode imputation on the categorical ones \n",
    "* we will discretize the continuous features by partitioning them into 5 bins (notice that this value can also be tuned)\n",
    "* we will perform ordinal encoding of the categorical features\n",
    "* we will merge the datasets, and train categorical Naive Bayes (with Laplace smoothing)  on these data\n",
    "* the test data will also be transformed, and used for the evaluation of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data import and train-test set split (with same parameters used in the Data Preprocessing notebook)\n",
    "from sklearn.model_selection import train_test_split\n",
    "heart_data = pd.read_csv(\"data/heart_data/heart.csv\")\n",
    "X = heart_data.drop('AHD', axis=1)\n",
    "y = heart_data['AHD']\n",
    "y_num = pd.factorize(y)[0]\n",
    "X.rename(columns={'Unnamed: 0':'ID'}, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_num, test_size=0.2, stratify=y_num, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Separate continuous from categorical (and binary) features\n",
    "X_train_cat = X_train.select_dtypes(include=['object'])\n",
    "X_test_cat = X_test.select_dtypes(include=['object'])\n",
    "X_train_num = X_train.select_dtypes(include=['int64', 'float64'])\n",
    "X_test_num = X_test.select_dtypes(include=['int64', 'float64'])\n",
    "is_binary = [len(X_train_num[x].value_counts())==2 for x in X_train_num.columns]\n",
    "X_train_cont = X_train_num.iloc[:, is_binary==np.repeat(False, len(is_binary))]\n",
    "X_test_cont = X_test_num.iloc[:, is_binary==np.repeat(False, len(is_binary))]\n",
    "X_train_bin = X_train_num.iloc[:, is_binary==np.repeat(True, len(is_binary))]\n",
    "X_test_bin = X_test_num.iloc[:, is_binary==np.repeat(True, len(is_binary))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train_cont_imp = pd.DataFrame(num_imputer.fit_transform(X_train_cont))\n",
    "X_train_cat_imp = pd.DataFrame(cat_imputer.fit_transform(X_train_cat))\n",
    "X_test_cont_imp = pd.DataFrame(num_imputer.transform(X_test_cont))\n",
    "X_test_cat_imp = pd.DataFrame(cat_imputer.transform(X_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Encoding Categorical Features \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_enc = OrdinalEncoder()\n",
    "X_train_cat_ord_enc = pd.DataFrame(ord_enc.fit_transform(X_train_cat_imp))\n",
    "X_test_cat_ord_enc = pd.DataFrame(ord_enc.transform(X_test_cat_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Discretizing Numeric Features - Use 5 bins per feature, uniformly partitioned\n",
    "# Feature 4, 7 and 8 are not discretized, as they only take on a small number of values distinct values\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "k_bins_disc = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform' )\n",
    "X_train_cont_disc = pd.DataFrame(k_bins_disc.fit_transform(X_train_cont_imp))\n",
    "X_test_cont_disc = pd.DataFrame(k_bins_disc.transform(X_test_cont_imp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6. Merging the datasets\n",
    "X_train_cont_disc.index = X_train.index\n",
    "X_train_cat_ord_enc.index = X_train.index \n",
    "X_test_cont_disc.index = X_test.index\n",
    "X_test_cat_ord_enc.index = X_test.index \n",
    "y_train= pd.Series(y_train, index = X_train.index)\n",
    "y_test = pd.Series(y_test, index = X_test.index)\n",
    "X_train_processed = pd.concat([X_train_cont_disc,X_train_cat_ord_enc, X_train_bin], axis=1)\n",
    "X_test_processed = pd.concat([X_test_cont_disc, X_test_cat_ord_enc, X_test_bin], axis=1 )\n",
    "# Update variable names \n",
    "new_var_names = list(X_train_cont.columns)\n",
    "new_var_names = new_var_names\n",
    "new_var_names = new_var_names + list(X_train_cat)\n",
    "new_var_names = new_var_names + list(X_train_bin.columns)\n",
    "X_train_processed.columns = new_var_names\n",
    "X_test_processed.columns = new_var_names\n",
    "X_train_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is ready for Categorical Naive Bayes. Let's train it and explore its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Implementing Categorical Naive Bayes\n",
    "cnb = CategoricalNB(alpha=1)\n",
    "cnb.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-specific counts: \n",
    "cnb.category_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on training set:\n",
    "cnb.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutation on test set:\n",
    "cnb.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the loss of information in the continuous data occured with the discretization step, Naive Bayes leads to a test set accuracy that is comparable with the one of Logistic Regression and Linear SVM.\n",
    "\n",
    "* As an exercise, try to tune the parameter $\\alpha$ and the number of bins for the discretization of continuous features (with cross-validation), and try to see if they can further improve the test accuracy of Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Naive Bayes: Other Remarks\n",
    "* The Naive Bayes model is simple to intepret (we need to evaluate the feature distributions within each class)\n",
    "* By means of the conditional independence assumption, the model needs to learn only a number of parameters linearly proportional to $p$, and thus it is computationally efficient in high dimensions\n",
    "* It supports multi-class classification\n",
    "* It is effective for predictions, as posterior probabilities need to assign the observations to the right side of the boundary, and need not be completely accurate \n",
    "* Naive Bayes, given its good performance in high dimensions, is widely used in document classification and spam e-mail detection\n",
    "* More information about the theory of Naive Bayes can be found in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
    "* If the conditional independence (and Gaussian, in Gaussian Naive Bayes) assumption holds, the model reaches the best possible accuracy... \n",
    "* ...but this is rarely the case, and to the extent that these assumptions are violated, performance of Naive Bayes will deteriorate. In this plot you can see an example where the conditional independence is clearly violated: \n",
    "\n",
    "\n",
    " <img src=\"./img/naive_bayes/naive_bayes_assumptions.png\" width=\"400\" height=\"50\"/>\n",
    "\n",
    "* It does not require intensive pre-processing: continuous feature need not be scaled, and categorical ones need not be encoded (although `scikit-learn` expects categories encoded as integers in categorical Naive Bayes)\n",
    "* It is possible to show (mathematically) that, when the variances of the features are the same for all classes, Naive Bayes builds a linear boundary (indeed, in the limit it provides the same boundary as logistic regression). When the variances differ, the boundary becomes quadratic: \n",
    "\n",
    "<img src=\"./img/naive_bayes/naive_bayes_linear_boundary.png\" width=\"250\" height=\"50\"/>\n",
    "<img src=\"./img/naive_bayes/naive_bayes_nonlinear_boundary.png\" width=\"250\" height=\"50\"/>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
